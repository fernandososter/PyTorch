{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4127308,"sourceType":"datasetVersion","datasetId":2438863}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-24T10:27:31.227060Z","iopub.execute_input":"2024-06-24T10:27:31.227712Z","iopub.status.idle":"2024-06-24T10:27:31.236103Z","shell.execute_reply.started":"2024-06-24T10:27:31.227676Z","shell.execute_reply":"2024-06-24T10:27:31.235140Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"/kaggle/input/house-price/1553768847-housing.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/house-price/1553768847-housing.csv\")\nis_missing_attr = df.isna()\nn_missing_attr = is_missing_attr.sum(axis=1)\ndf[n_missing_attr > 0]\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T10:27:31.237678Z","iopub.execute_input":"2024-06-24T10:27:31.237931Z","iopub.status.idle":"2024-06-24T10:27:31.294846Z","shell.execute_reply.started":"2024-06-24T10:27:31.237909Z","shell.execute_reply":"2024-06-24T10:27:31.293924Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n290      -122.16     37.77                  47         1256             NaN   \n341      -122.17     37.75                  38          992             NaN   \n538      -122.28     37.78                  29         5154             NaN   \n563      -122.24     37.75                  45          891             NaN   \n696      -122.10     37.69                  41          746             NaN   \n...          ...       ...                 ...          ...             ...   \n20267    -119.19     34.20                  18         3620             NaN   \n20268    -119.18     34.19                  19         2393             NaN   \n20372    -118.88     34.17                  15         4260             NaN   \n20460    -118.75     34.29                  17         5512             NaN   \n20484    -118.72     34.28                  17         3051             NaN   \n\n       population  households  median_income ocean_proximity  \\\n290           570         218         4.3750        NEAR BAY   \n341           732         259         1.6196        NEAR BAY   \n538          3741        1273         2.5762        NEAR BAY   \n563           384         146         4.9489        NEAR BAY   \n696           387         161         3.9063        NEAR BAY   \n...           ...         ...            ...             ...   \n20267        3171         779         3.3409      NEAR OCEAN   \n20268        1938         762         1.6953      NEAR OCEAN   \n20372        1701         669         5.1033       <1H OCEAN   \n20460        2734         814         6.6073       <1H OCEAN   \n20484        1705         495         5.7376       <1H OCEAN   \n\n       median_house_value  \n290                161900  \n341                 85100  \n538                173400  \n563                247100  \n696                178400  \n...                   ...  \n20267              220500  \n20268              167400  \n20372              410700  \n20460              258100  \n20484              218600  \n\n[207 rows x 10 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>ocean_proximity</th>\n      <th>median_house_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>290</th>\n      <td>-122.16</td>\n      <td>37.77</td>\n      <td>47</td>\n      <td>1256</td>\n      <td>NaN</td>\n      <td>570</td>\n      <td>218</td>\n      <td>4.3750</td>\n      <td>NEAR BAY</td>\n      <td>161900</td>\n    </tr>\n    <tr>\n      <th>341</th>\n      <td>-122.17</td>\n      <td>37.75</td>\n      <td>38</td>\n      <td>992</td>\n      <td>NaN</td>\n      <td>732</td>\n      <td>259</td>\n      <td>1.6196</td>\n      <td>NEAR BAY</td>\n      <td>85100</td>\n    </tr>\n    <tr>\n      <th>538</th>\n      <td>-122.28</td>\n      <td>37.78</td>\n      <td>29</td>\n      <td>5154</td>\n      <td>NaN</td>\n      <td>3741</td>\n      <td>1273</td>\n      <td>2.5762</td>\n      <td>NEAR BAY</td>\n      <td>173400</td>\n    </tr>\n    <tr>\n      <th>563</th>\n      <td>-122.24</td>\n      <td>37.75</td>\n      <td>45</td>\n      <td>891</td>\n      <td>NaN</td>\n      <td>384</td>\n      <td>146</td>\n      <td>4.9489</td>\n      <td>NEAR BAY</td>\n      <td>247100</td>\n    </tr>\n    <tr>\n      <th>696</th>\n      <td>-122.10</td>\n      <td>37.69</td>\n      <td>41</td>\n      <td>746</td>\n      <td>NaN</td>\n      <td>387</td>\n      <td>161</td>\n      <td>3.9063</td>\n      <td>NEAR BAY</td>\n      <td>178400</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>20267</th>\n      <td>-119.19</td>\n      <td>34.20</td>\n      <td>18</td>\n      <td>3620</td>\n      <td>NaN</td>\n      <td>3171</td>\n      <td>779</td>\n      <td>3.3409</td>\n      <td>NEAR OCEAN</td>\n      <td>220500</td>\n    </tr>\n    <tr>\n      <th>20268</th>\n      <td>-119.18</td>\n      <td>34.19</td>\n      <td>19</td>\n      <td>2393</td>\n      <td>NaN</td>\n      <td>1938</td>\n      <td>762</td>\n      <td>1.6953</td>\n      <td>NEAR OCEAN</td>\n      <td>167400</td>\n    </tr>\n    <tr>\n      <th>20372</th>\n      <td>-118.88</td>\n      <td>34.17</td>\n      <td>15</td>\n      <td>4260</td>\n      <td>NaN</td>\n      <td>1701</td>\n      <td>669</td>\n      <td>5.1033</td>\n      <td>&lt;1H OCEAN</td>\n      <td>410700</td>\n    </tr>\n    <tr>\n      <th>20460</th>\n      <td>-118.75</td>\n      <td>34.29</td>\n      <td>17</td>\n      <td>5512</td>\n      <td>NaN</td>\n      <td>2734</td>\n      <td>814</td>\n      <td>6.6073</td>\n      <td>&lt;1H OCEAN</td>\n      <td>258100</td>\n    </tr>\n    <tr>\n      <th>20484</th>\n      <td>-118.72</td>\n      <td>34.28</td>\n      <td>17</td>\n      <td>3051</td>\n      <td>NaN</td>\n      <td>1705</td>\n      <td>495</td>\n      <td>5.7376</td>\n      <td>&lt;1H OCEAN</td>\n      <td>218600</td>\n    </tr>\n  </tbody>\n</table>\n<p>207 rows × 10 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.dropna(inplace=True)\ndf\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# shuffling DS","metadata":{}},{"cell_type":"code","source":"shuffled = df.sample(frac=1, random_state=1).reset_index(drop=True)\nshuffled","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# splitting","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split\n\ntrainval, test = train_test_split(shuffled, test_size=0.16, shuffle=False)\ntrain, val = train_test_split(trainval, test_size=0.2, shuffle=False)\n\n\ndf_target_train = train['median_house_value']\ntrain.drop(\"median_house_value\",axis=1,inplace=True)\n\ndf_target_test = test['median_house_value']\ntest.drop(\"median_house_value\",axis=1,inplace=True)\n\ndf_target_val = val['median_house_value']\nval.drop(\"median_house_value\",axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test),len(train),len(val)\nprint(val[\"ocean_proximity\"].unique())\nprint(train[\"ocean_proximity\"].unique()) \nprint(test[\"ocean_proximity\"].unique())\n# no dataset de trainamento tem um ISLAND que nao tem no test e validation. \n\nisland_regs = train[ train[\"ocean_proximity\"] == \"ISLAND\"]\ntrain.drop(island_regs.index,inplace=True)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(val[\"ocean_proximity\"].unique())\nprint(train[\"ocean_proximity\"].unique()) \nprint(test[\"ocean_proximity\"].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Normaling dataset\nsplitting data in categorical, ordinal and continuous","metadata":{}},{"cell_type":"code","source":"display(train)\ncontin_attr = [\"housing_median_age\",\"total_rooms\",\"total_bedrooms\",\"population\",\"households\",\"median_income\"]\ncate_attr = [\"ocean_proximity\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Normalizing continious values","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = {\"train\":None, \"test\":None, \"val\":None}\n\nscaler = StandardScaler()\nencoder = OneHotEncoder(sparse=False)\n\n# Fit the scaler on the training data\nscaler.fit(train[contin_attr])\n\nencoder = OneHotEncoder(sparse=False)\n\n\ndef normalizing(df): \n    return scaler.transform(df)\n\ndef encoding(df):\n    return encoder.fit_transform(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_cont = pd.DataFrame(normalizing(train[contin_attr])) \ntrain_cat = pd.DataFrame(encoding(train[cate_attr]))\ndataset[\"train\"] = pd.concat([train_cont,train_cat],axis=1)\n\ntest_cont = pd.DataFrame(normalizing(test[contin_attr])) \ntest_cat = pd.DataFrame(encoding(test[cate_attr]))\ndataset[\"test\"] = pd.concat([test_cont,test_cat],axis=1)\n\nval_cont = pd.DataFrame(normalizing(val[contin_attr])) \nval_cat = pd.DataFrame(encoding(val[cate_attr]))\ndataset[\"val\"] = pd.concat([val_cont,val_cat],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"code","source":"len(dataset[\"train\"]),len(dataset[\"test\"]),len(dataset[\"val\"])\ndataset[\"train\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# dataset","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader\nimport numpy as np\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert numpy arrays to PyTorch tensors\nfeatures_tensor = torch.tensor(dataset[\"train\"].to_numpy(),dtype=torch.float32)\ntarget_tensor = torch.tensor(df_target_train.to_numpy(),dtype=torch.float32)\n\n# VALIDATION\nval_features_tensor = torch.tensor(dataset[\"val\"].to_numpy(),dtype=torch.float32)\nval_target_tensor = torch.tensor(df_target_val.to_numpy(),dtype=torch.float32)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tensor_dataset = TensorDataset(features_tensor, target_tensor)\ndataloader = DataLoader(tensor_dataset, batch_size=32, shuffle=True)\n\n\n#VALIDATION\nval_tensor_dataset = TensorDataset(val_features_tensor, val_target_tensor)\nval_dataloader = DataLoader(val_tensor_dataset, batch_size=32, shuffle=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 4: Iterate through the DataLoader\nfor batch_features, batch_targets in dataloader:\n    print(\"Batch features:\", batch_features)\n    print(\"Batch targets:\", batch_targets)\n    break  # Just to show the first batch\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Linear Regression","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\ntorch.manual_seed(13)\nlinear = nn.Linear(in_features=11, out_features=1, bias=True)\n\nmodel = nn.Sequential()\nmodel.add_module('layer1', linear)\nmodel.state_dict(),list(model.parameters())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bad_predictions = model(batch_features)\nbad_predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bad_predictions - batch_targets\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss - MSE","metadata":{}},{"cell_type":"code","source":"loss_fn = nn.MSELoss()\nloss = loss_fn(bad_predictions, batch_targets)\nloss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# aplicando o gradient descent automatico\nloss.backward()\nmodel.layer1.weight.grad, model.layer1.bias.grad\n# -470863.1250 é uma tx de erro muito menor que 6.6361e+10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optimizer ","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\noptimizer = optim.Adam(params=model.parameters(), lr=1.1)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_epochs = 100\nlosses = torch.empty(n_epochs) \nmodel.to(device) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(n_epochs):\n    batch_losses = torch.empty(len(dataloader)) \n    #print(epoch)\n    for i, (batch_features, batch_targets) in enumerate(dataloader):\n        model.train()\n        batch_features = batch_features.to(device)\n        batch_targets = batch_targets.to(device) \n\n        # Step 1 - forward pass\n        predictions = model(batch_features) \n\n        # Step 2 - computing the loss\n        loss = loss_fn(predictions, batch_targets) \n\n        # Step 3 - computing the gradients\n        loss.backward()\n        batch_losses[i] = loss.item() \n\n        # Step 4 - updating parameters and zeroing gradients\n        optimizer.step()\n        optimizer.zero_grad() \n\n    losses[epoch] = batch_losses.mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mostrando a curva de aprendizado. Entre 900 e 1000 comecou a entrar em flat. ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(n_epochs):\n    batch_losses = torch.empty(len(dataloader)) \n    #print(epoch)\n    for i, (batch_features, batch_targets) in enumerate(dataloader):\n        model.train()\n        batch_features = batch_features.to(device)\n        batch_targets = batch_targets.to(device) \n\n        # Step 1 - forward pass\n        predictions = model(batch_features) \n\n        # Step 2 - computing the loss\n        loss = loss_fn(predictions, batch_targets) \n\n        # Step 3 - computing the gradients\n        loss.backward()\n        batch_losses[i] = loss.item() \n\n        # Step 4 - updating parameters and zeroing gradients\n        optimizer.step()\n        optimizer.zero_grad() \n\n    losses[epoch] = batch_losses.mean()\n    \n    ## VALIDATION\n    batch_losses = torch.empty(len(val_dataloader)) \n    \n    with torch.inference_mode():\n        for i, (val_features, val_targets) in enumerate(val_dataloader):\n            # Sprinkle\n            model.eval() # Chamando o Eval() para ativar o evaluation\n\n            val_features= val_features.to(device)\n            val_targets = val_targets.to(device) \n\n            # Step 1 - forward pass\n            predictions = model(val_features) \n\n            # Step 2 - computing the loss\n            loss = loss_fn(predictions, val_targets)\n            batch_losses[i] = loss.item() \n\n    val_losses[epoch] = batch_losses.mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(losses, label='Training')\nplt.plot(val_losses, label='Validation')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.yscale('log')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}